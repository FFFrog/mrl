<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>PyTorch AMP Mechanism | Jiawei Li`s Blog</title><meta name="author" content="Jiawei Li,ljw1101.vip@gmail.com"><meta name="copyright" content="Jiawei Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Automatic mixed precision(AMP) 自动混合精度训练与推理，自动决定哪些算子以float16运行，哪些以float32运行，从而在降低内存以及带宽，提高计算性能的同时，减少精度损失。  背景硬件Nvidia 在 Volta 架构中首次引入 Tensor Core 单元，来支持 FP32 和 FP16 混合精度计算  FMA: Fused Multiply-Add 的操作">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch AMP Mechanism">
<meta property="og:url" content="https://fffrog.github.io/mrl/2023/11/13/PyTorch-AMP-Mechanism/index.html">
<meta property="og:site_name" content="Jiawei Li&#96;s Blog">
<meta property="og:description" content="Automatic mixed precision(AMP) 自动混合精度训练与推理，自动决定哪些算子以float16运行，哪些以float32运行，从而在降低内存以及带宽，提高计算性能的同时，减少精度损失。  背景硬件Nvidia 在 Volta 架构中首次引入 Tensor Core 单元，来支持 FP32 和 FP16 混合精度计算  FMA: Fused Multiply-Add 的操作">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.mji.rip/2023/11/08/d1245194556cd938b7176c269d1adacf.jpeg">
<meta property="article:published_time" content="2023-11-13T02:39:33.000Z">
<meta property="article:modified_time" content="2023-11-24T09:19:26.736Z">
<meta property="article:author" content="Jiawei Li">
<meta property="article:tag" content="AMP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.mji.rip/2023/11/08/d1245194556cd938b7176c269d1adacf.jpeg"><link rel="shortcut icon" href="/mrl/img/favicon.png"><link rel="canonical" href="https://fffrog.github.io/mrl/2023/11/13/PyTorch-AMP-Mechanism/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/mrl/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/mrl/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: Jiawei Li","link":"Link: ","source":"Source: Jiawei Li`s Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'PyTorch AMP Mechanism',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-11-24 17:19:26'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.mji.rip/2023/11/08/d1245194556cd938b7176c269d1adacf.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/mrl/archives/"><div class="headline">Articles</div><div class="length-num">6</div></a><a href="/mrl/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/mrl/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/mrl/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.mji.rip/2023/11/08/85716b57916f4f58499316e28a7ac021.jpeg')"><nav id="nav"><span id="blog-info"><a href="/mrl/" title="Jiawei Li`s Blog"><span class="site-name">Jiawei Li`s Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/mrl/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">PyTorch AMP Mechanism</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-13T02:39:33.000Z" title="Created 2023-11-13 10:39:33">2023-11-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-24T09:19:26.736Z" title="Updated 2023-11-24 17:19:26">2023-11-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/mrl/categories/PyTorch/">PyTorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="PyTorch AMP Mechanism"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p>Automatic mixed precision(AMP) 自动混合精度训练与推理，自动决定哪些算子以float16运行，哪些以float32运行，从而在降低内存以及带宽，提高计算性能的同时，减少精度损失。</p>
</blockquote>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h3><p>Nvidia 在 Volta 架构中首次引入 Tensor Core 单元，来支持 FP32 和 FP16 混合精度计算</p>
<blockquote>
<p>FMA: Fused Multiply-Add 的操作，通俗一点就是一个加乘操作的融合</p>
</blockquote>
<p><img src="/mrl/images/Volta-Tensor-Core.gif" alt="CUDA Core VS Tensor Core"></p>
<p>Cuda Core:<br>Cuda Core是全能通用型的浮点运算单元，单GPU时钟周期完成一次FMA操作。</p>
<p>Tensor Core:<br>Tensor Core是专为执行深度学习矩阵运算而设计的专用执行单元，单GPU时钟周期完成一次矩阵相乘（以 4 * 4矩阵为例，单GPU时钟周期可以完成64次 FMA）</p>
<p><img src="https://i.mji.rip/2023/11/13/fe80e1e7a11d6b5325c1619339acba71.png" alt="Mul_Add"></p>
<p>每一代Tensor Core支持的计算类型如下：<br><img src="https://i.mji.rip/2023/11/13/13541dad95c1e70fcb304a9abc6fbb78.png" alt="Tensor Core"></p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p><strong>Transform</strong>模型以及传统<strong>CNN</strong>模型等充斥着大量矩阵乘加运算</p>
<h2 id="前世今生"><a href="#前世今生" class="headerlink" title="前世今生"></a>前世今生</h2><h3 id="Nvidia-apex项目"><a href="#Nvidia-apex项目" class="headerlink" title="Nvidia apex项目"></a>Nvidia apex项目</h3><p>为了释放Tensor Core的巨大性能，Nvidia 于 2018 年提出一个 PyTorch 拓展 <a target="_blank" rel="noopener" href="https://github.com/nvidia/apex">apex</a>，来支持模型参数自动混合精度训练和推理，用户只需要安装该插件(PyTorch 1.6之前)便可以使用Tensor Core的能力，该仓库只要包括以下几个特性：</p>
<ol>
<li>Amp: Automatic Mixed Precision</li>
<li>Distributed Training(like DDP)</li>
<li>Synchronized Batch Normalization(like torch.nn.SyncBN)</li>
<li>Checkpointing(like checkpoint in PyTorch, for inference or resuming training)</li>
</ol>
<h3 id="PyTorch-1-6-内部集成-amp"><a href="#PyTorch-1-6-内部集成-amp" class="headerlink" title="PyTorch 1.6 内部集成 amp"></a>PyTorch 1.6 内部集成 amp</h3><p>2019年，Nvidia 在PyTorch 提交了一个 <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/issues/25081">Feature Discussion</a>，主要目的是想将apex的能力集成到PyTorch社区，也就是现在的<strong>torch.cuda.amp</strong>模块，基于的主要原因就是方便用户使用以及其真实的性能提升，最终跟随PyTorch 1.6版本原生发布。</p>
<h2 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding: UTF-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> GradScaler</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">3</span>).cuda()</span><br><span class="line">y = torch.randn(<span class="number">3</span>, <span class="number">3</span>).cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        ret = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = LinearModel().cuda()</span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">scaler = GradScaler()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">with</span> torch.autocast(device_type=<span class="string">&#x27;cuda&#x27;</span>, dtype=torch.float16):</span><br><span class="line">        ret = model(x)</span><br><span class="line">        loss = criterion(ret, y)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    scaler.scale(loss).backward()</span><br><span class="line">    scaler.step(optimizer)</span><br><span class="line">    scaler.update()</span><br></pre></td></tr></table></figure>

<h2 id="设计与实现"><a href="#设计与实现" class="headerlink" title="设计与实现"></a>设计与实现</h2><h3 id="基于-Dispatcher-的注册以及分发-autocast"><a href="#基于-Dispatcher-的注册以及分发-autocast" class="headerlink" title="基于 Dispatcher 的注册以及分发(autocast)"></a>基于 Dispatcher 的注册以及分发(autocast)</h3><p><img src="https://i.miji.bid/2023/11/24/0fbd960495c06d2bdb5f33c3556a0e1a.png" alt="AMP Overview"></p>
<p>TLS中的DispatchKeySet中插入Autocast Key</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    <span class="keyword">with</span> torch.autocast(device_type=<span class="string">&#x27;cuda&#x27;</span>, dtype=torch.float16):</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p><img src="https://i.miji.bid/2023/11/24/c831f58e0e26f9b8624776717405d7a4.png" alt="Include"></p>
<p>前向forward开始</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    ret = model(x)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p><img src="https://i.miji.bid/2023/11/24/6fd38607e311af2c866cab6c32274fc9.png" alt="Before"></p>
<p>Autocast 的CUDA实现（lower_precision_fp)</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;</span><br><span class="line">    c10::DeviceType device_type,</span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">Redispatch</span>,</span><br><span class="line">    Redispatch* F,</span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">Ret</span>,</span><br><span class="line">    <span class="keyword">class</span>... Args&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">WrapFunction_</span>&lt;</span><br><span class="line">    CastPolicy::lower_precision_fp,</span><br><span class="line">    device_type,</span><br><span class="line">    Redispatch,</span><br><span class="line">    F,</span><br><span class="line">    Ret,</span><br><span class="line">    guts::typelist::typelist&lt;Args...&gt;&gt; &#123;</span><br><span class="line">  <span class="function"><span class="type">static</span> Ret <span class="title">call</span><span class="params">(Args... args)</span> </span>&#123;</span><br><span class="line">    c10::<span class="function">impl::ExcludeDispatchKeyGuard <span class="title">no_autocast</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        get_autocast_dispatch_key_from_device_type(device_type))</span></span>;</span><br><span class="line">    <span class="keyword">return</span> (*F)(<span class="built_in">cached_cast</span>(</span><br><span class="line">        <span class="built_in">get_lower_precision_fp_from_device_type</span>(device_type),</span><br><span class="line">        args,</span><br><span class="line">        device_type)...);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>防止 autocast 无限递归</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c10::<span class="function">impl::ExcludeDispatchKeyGuard <span class="title">no_autocast</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    get_autocast_dispatch_key_from_device_type(device_type))</span></span>;</span><br></pre></td></tr></table></figure>

<p><img src="https://i.miji.bid/2023/11/24/5669ecec5cff9b8078cb2a2454d77115.png" alt="Exclude"></p>
<p>转向后续处理(autograd, 算子真实的cuda实现)</p>
<p><img src="https://i.miji.bid/2023/11/24/a740c9b564e13c9c53678a2c1189e612.png" alt="After"></p>
<h3 id="如何解决float16自身精度引入的溢出问题？"><a href="#如何解决float16自身精度引入的溢出问题？" class="headerlink" title="如何解决float16自身精度引入的溢出问题？"></a>如何解决float16自身精度引入的溢出问题？</h3><p><img src="https://i.mji.rip/2023/11/13/5633873758e6e1b7a984118d0d3104af.png" alt="Float32 vs Float16"></p>
<p>Float16 表示的范围较窄，大量非 0 梯度会遇到溢出问题。</p>
<h3 id="梯度缩放-GradScaler"><a href="#梯度缩放-GradScaler" class="headerlink" title="梯度缩放(GradScaler)"></a>梯度缩放(GradScaler)</h3><p><img src="https://i.miji.bid/2023/11/24/4b0719b29789d1ac24d02e9400d6455d.png" alt="Percentage of all activation gradient values"></p>
<p>解决方法：<br>在反向传播之前，给loss乘以一个scaler factor，将会导致反向传播中的每个grad都乘以了相同的scaler factor，变相增加了float16可表示小数的范围，从而解决float16精度丢失的问题。</p>
<p>相关代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scaler.scale(loss).backward()</span><br><span class="line">scaler.step(optimizer)</span><br><span class="line">scaler.update()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaler.scale(loss).backward()</span><br></pre></td></tr></table></figure>

<p>将loss乘以一个默认的scaler factor,然后按照正常流程反向传播，生成每个grad</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaler.step(optimizer)</span><br></pre></td></tr></table></figure>

<p>将每个grad都除以scaler factor，进行梯度还原，同时记录梯度还原过程中是否存在inf&#x2F;nan情况</p>
<ul>
<li>如果存在inf&#x2F;nan，本次迭代参数将不更新</li>
<li>如果不存在inf&#x2F;nan，本次迭代参数将正不更新</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scaler.scale(loss).backward()</span><br><span class="line">scaler.step(optimizer)</span><br><span class="line">scaler.update()</span><br></pre></td></tr></table></figure>

<p>根据上一步是否存在inf&#x2F;nan的情况，动态调整scaler factor(每隔 <strong>_growth_interval</strong> 个 迭代周期)</p>
<ul>
<li>如果存在inf&#x2F;nan，scaler factor 乘以 一个小于1的系数 <strong>_growth_factor</strong></li>
<li>如果不存在inf&#x2F;nan，scaler factor 乘以 一个大于1的系数 <strong>_backoff_factor</strong></li>
</ul>
<p>以上三个参数均有默认值，也可以在初始化 <strong>GradScaler</strong> 时自行设置</p>
<h2 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h2><h3 id="AMP-是否依赖-Tensor-Core"><a href="#AMP-是否依赖-Tensor-Core" class="headerlink" title="AMP 是否依赖 Tensor Core"></a>AMP 是否依赖 Tensor Core</h3><p>不依赖，不支持 Tensor Core的GPU也可以使用 <strong>AMP</strong>，内存会有降低，但是计算性能不保证</p>
<h3 id="AMP-复制两份参数，为什么还能降低存储"><a href="#AMP-复制两份参数，为什么还能降低存储" class="headerlink" title="AMP 复制两份参数，为什么还能降低存储"></a>AMP 复制两份参数，为什么还能降低存储</h3><p>PyTorch 模型训练时主要的内存占用大体分为以下几个方面：</p>
<ol>
<li>模型参数：包括权重以及输入</li>
<li>优化器: Adam优化器 (momentum,variance)&#x2F;Per Grad，其他优化器类似</li>
<li>AMP: 复制 float16 输入以及权重</li>
<li>激活值：计算中间值，用来为backward做准备</li>
<li>梯度：每个参数的梯度值</li>
</ol>
<p>其中，第三项会增加内存使用(每个参数新增两字节消耗)，而第4，5项由于以半精度存储（相对于单精度，激活值和权重加起来减少了4字节消耗）</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://fffrog.github.io/mrl">Jiawei Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://fffrog.github.io/mrl/2023/11/13/PyTorch-AMP-Mechanism/">https://fffrog.github.io/mrl/2023/11/13/PyTorch-AMP-Mechanism/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/mrl/tags/AMP/">AMP</a></div><div class="post_share"><div class="social-share" data-image="https://i.mji.rip/2023/11/08/d1245194556cd938b7176c269d1adacf.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/mrl/2025/01/20/The-Introduction-of-PyTorch-Compiler/" title="The Introduction of PyTorch Compiler"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">The Introduction of PyTorch Compiler</div></div></a></div><div class="next-post pull-right"><a href="/mrl/2023/11/09/PyTorch-Tensor-Mechanism/" title="PyTorch Tensor Mechanism"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">PyTorch Tensor Mechanism</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.mji.rip/2023/11/08/d1245194556cd938b7176c269d1adacf.jpeg" onerror="this.onerror=null;this.src='/mrl/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Jiawei Li</div><div class="author-info__description">Stay Hungry, Stay Foolish</div></div><div class="card-info-data site-data is-center"><a href="/mrl/archives/"><div class="headline">Articles</div><div class="length-num">6</div></a><a href="/mrl/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/mrl/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/FFFrog" target="_blank" title="Github"><i class="fab fa-github" style="color: #hdhfbb;"></i></a><a class="social-icon" href="mailto:ljw1101.vip@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #000000;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Happy Everyday</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6"><span class="toc-number">1.1.</span> <span class="toc-text">硬件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.2.</span> <span class="toc-text">模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F"><span class="toc-number">2.</span> <span class="toc-text">前世今生</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Nvidia-apex%E9%A1%B9%E7%9B%AE"><span class="toc-number">2.1.</span> <span class="toc-text">Nvidia apex项目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch-1-6-%E5%86%85%E9%83%A8%E9%9B%86%E6%88%90-amp"><span class="toc-number">2.2.</span> <span class="toc-text">PyTorch 1.6 内部集成 amp</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">用法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.</span> <span class="toc-text">设计与实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E-Dispatcher-%E7%9A%84%E6%B3%A8%E5%86%8C%E4%BB%A5%E5%8F%8A%E5%88%86%E5%8F%91-autocast"><span class="toc-number">4.1.</span> <span class="toc-text">基于 Dispatcher 的注册以及分发(autocast)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3float16%E8%87%AA%E8%BA%AB%E7%B2%BE%E5%BA%A6%E5%BC%95%E5%85%A5%E7%9A%84%E6%BA%A2%E5%87%BA%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">4.2.</span> <span class="toc-text">如何解决float16自身精度引入的溢出问题？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%BC%A9%E6%94%BE-GradScaler"><span class="toc-number">4.3.</span> <span class="toc-text">梯度缩放(GradScaler)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#QA"><span class="toc-number">5.</span> <span class="toc-text">QA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#AMP-%E6%98%AF%E5%90%A6%E4%BE%9D%E8%B5%96-Tensor-Core"><span class="toc-number">5.1.</span> <span class="toc-text">AMP 是否依赖 Tensor Core</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AMP-%E5%A4%8D%E5%88%B6%E4%B8%A4%E4%BB%BD%E5%8F%82%E6%95%B0%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%83%BD%E9%99%8D%E4%BD%8E%E5%AD%98%E5%82%A8"><span class="toc-number">5.2.</span> <span class="toc-text">AMP 复制两份参数，为什么还能降低存储</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/mrl/2025/01/20/The-Introduction-of-PyTorch-Compiler/" title="The Introduction of PyTorch Compiler">The Introduction of PyTorch Compiler</a><time datetime="2025-01-20T02:20:00.000Z" title="Created 2025-01-20 10:20:00">2025-01-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/mrl/2023/11/13/PyTorch-AMP-Mechanism/" title="PyTorch AMP Mechanism">PyTorch AMP Mechanism</a><time datetime="2023-11-13T02:39:33.000Z" title="Created 2023-11-13 10:39:33">2023-11-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/mrl/2023/11/09/PyTorch-Tensor-Mechanism/" title="PyTorch Tensor Mechanism">PyTorch Tensor Mechanism</a><time datetime="2023-11-09T02:30:48.000Z" title="Created 2023-11-09 10:30:48">2023-11-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/mrl/2023/11/09/PyTorch-Operator-Register/" title="PyTorch Operator Register">PyTorch Operator Register</a><time datetime="2023-11-09T02:27:53.000Z" title="Created 2023-11-09 10:27:53">2023-11-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/mrl/2023/11/09/PyTorch-Dispatcher-Mechanism/" title="PyTorch Dispatcher Mechansim">PyTorch Dispatcher Mechansim</a><time datetime="2023-11-09T02:17:25.000Z" title="Created 2023-11-09 10:17:25">2023-11-09</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Jiawei Li</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/mrl/js/utils.js"></script><script src="/mrl/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>