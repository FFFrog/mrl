<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Understanding PyTorch Graph Mode | Jiawei Li`s Blog</title><meta name="author" content="Jiawei Li,ljw1101.vip@gmail.com"><meta name="copyright" content="Jiawei Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="简介PyTorch 最大的优点就是其 Eager 模式下的灵活性，赋予了用户开发、调试方面无与伦比的便利；但是 Eager 模式在生产方面却有着巨大的先天缺陷，由此 PyTorch 从 1.0 版本就致力于推出 PyTorch 的图模式，但是由于 Eager 模式的代码并非完备的构图语义，无法直接将其转换成等价的静态图，因此 PyTorch 前前后后进行了多次尝试，直至 PyTorch 2.0 才">
<meta property="og:type" content="article">
<meta property="og:title" content="Understanding PyTorch Graph Mode">
<meta property="og:url" content="https://fffrog.github.io/mrl/2025/06/16/Advanced-PyTorch-Features-Graph-Mode/index.html">
<meta property="og:site_name" content="Jiawei Li&#96;s Blog">
<meta property="og:description" content="简介PyTorch 最大的优点就是其 Eager 模式下的灵活性，赋予了用户开发、调试方面无与伦比的便利；但是 Eager 模式在生产方面却有着巨大的先天缺陷，由此 PyTorch 从 1.0 版本就致力于推出 PyTorch 的图模式，但是由于 Eager 模式的代码并非完备的构图语义，无法直接将其转换成等价的静态图，因此 PyTorch 前前后后进行了多次尝试，直至 PyTorch 2.0 才">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.mji.rip/2023/11/08/d1245194556cd938b7176c269d1adacf.jpeg">
<meta property="article:published_time" content="2025-06-16T11:12:42.000Z">
<meta property="article:modified_time" content="2025-06-20T09:37:26.121Z">
<meta property="article:author" content="Jiawei Li">
<meta property="article:tag" content="Compiler">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.mji.rip/2023/11/08/d1245194556cd938b7176c269d1adacf.jpeg"><link rel="shortcut icon" href="/mrl/img/favicon.png"><link rel="canonical" href="https://fffrog.github.io/mrl/2025/06/16/Advanced-PyTorch-Features-Graph-Mode/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/mrl/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/mrl/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: Jiawei Li","link":"Link: ","source":"Source: Jiawei Li`s Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Understanding PyTorch Graph Mode',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-06-20 17:37:26'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.mji.rip/2023/11/08/d1245194556cd938b7176c269d1adacf.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/mrl/archives/"><div class="headline">Articles</div><div class="length-num">7</div></a><a href="/mrl/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/mrl/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/mrl/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.mji.rip/2023/11/08/85716b57916f4f58499316e28a7ac021.jpeg')"><nav id="nav"><span id="blog-info"><a href="/mrl/" title="Jiawei Li`s Blog"><span class="site-name">Jiawei Li`s Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/mrl/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/mrl/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Understanding PyTorch Graph Mode</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-06-16T11:12:42.000Z" title="Created 2025-06-16 19:12:42">2025-06-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-06-20T09:37:26.121Z" title="Updated 2025-06-20 17:37:26">2025-06-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/mrl/categories/PyTorch/">PyTorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Understanding PyTorch Graph Mode"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>PyTorch 最大的优点就是其 Eager 模式下的灵活性，赋予了用户开发、调试方面无与伦比的便利；但是 Eager 模式在生产方面却有着巨大的先天缺陷，由此 PyTorch 从 1.0 版本就致力于推出 PyTorch 的图模式，但是由于 Eager 模式的代码并非完备的构图语义，无法直接将其转换成等价的静态图，因此 PyTorch 前前后后进行了多次尝试，直至 PyTorch 2.0 才基本确定了以 torch.compile 为主路径的图模式。</p>
<p>本系列将围绕 torch.compiler 以下几个主要模块进行代码剖析:</p>
<ul>
<li>Dynamo</li>
<li>AOT Autograd</li>
<li>Inductor</li>
</ul>
<h2 id="图模式的前世今生"><a href="#图模式的前世今生" class="headerlink" title="图模式的前世今生"></a>图模式的前世今生</h2><p>在进行上述模块剖析之前，本篇文章先简单总结一下 PyTorch 这么多年在图模式领域的多种尝试，此处借用 PyTorch 核心贡献者 penguinwu 在 PyTorch 开发者论坛中的经典图片，更多详细信息请参考 <a target="_blank" rel="noopener" href="https://dev-discuss.pytorch.org/t/the-nuances-of-pytorch-graph-capture/501">The nuances of PyTorch Graph Capture</a>, 这张图片详细的介绍了 PyTorch 当前阶段各种图捕获能力之间的关联和区别。</p>
<p><img src="https://i.miji.bid/2025/01/21/bf31be13684ea036aa17a573399ee9bd.jpeg" alt="PyTorch 图捕获分类"></p>
<p>下面这张图片从 PyTorch(Python入口) 代码执行的角度，简单的描述了各种图捕获能力的入口位置，下面将结合上面两张图具体分析各种图捕获能力的原理以及优缺点。</p>
<p><img src="https://i.miji.bid/2025/06/20/6e4eea857426422bc9df549ea8c75ffa.png" alt="PyTorch 图模式层次图"></p>
<h3 id="torch-jit"><a href="#torch-jit" class="headerlink" title="torch.jit"></a>torch.jit</h3><p>PyTorch 在 v1.0.0 版本的时候首次尝试了静态图捕获，实现了一套 JIT 的能力，引入了 TorchScript IR、两种导出 TorchScript IR 的方式以及 IR 优化器和解释器，可以将 function 或者 nn.Module 转换成 ScriptFunction 或者 ScriptModule，并且可以在非 Python 环境下执行。这两种导出方式分别是</p>
<ul>
<li>torch.jit.trace</li>
<li>torch.jit.script</li>
</ul>
<p>其中，torch.jit.trace 也是 torch.onnx.export 捕获静态图的基础（PyTorch 2.x 之后，torch.onnx.export 已基于 torch.dynamo）</p>
<h4 id="torch-jit-trace"><a href="#torch-jit-trace" class="headerlink" title="torch.jit.trace"></a>torch.jit.trace</h4><p><code>torch.jit.trace</code> 是在 C++ 层面利用 DispatchKey <code>Tracer</code> 进行实现的，简单来说就是传入数据端到端运行一遍待捕获的方法或者模块，借助 <code>Tracer</code> DispatchKey 的高优先级在具体算子执行之前捕获静态图，以 <code>add.Tensor</code> 算子为例，详细参考下面代码（源码节选）：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">at::Tensor <span class="title">add_Tensor</span><span class="params">(c10::DispatchKeySet ks, <span class="type">const</span> at::Tensor &amp; self, <span class="type">const</span> at::Tensor &amp; other, <span class="type">const</span> at::Scalar &amp; alpha)</span> </span>&#123;</span><br><span class="line">  torch::jit::Node* node = <span class="literal">nullptr</span>;</span><br><span class="line">  std::shared_ptr&lt;jit::tracer::TracingState&gt; tracer_state;</span><br><span class="line">  <span class="keyword">if</span> (jit::tracer::<span class="built_in">isTracing</span>()) &#123;</span><br><span class="line">    tracer_state = jit::tracer::<span class="built_in">getTracingState</span>();</span><br><span class="line">    at::Symbol op_name;</span><br><span class="line">    op_name = c10::Symbol::<span class="built_in">fromQualString</span>(<span class="string">&quot;aten::add&quot;</span>);</span><br><span class="line">    node = tracer_state-&gt;<span class="built_in">createNode</span>(op_name, <span class="comment">/*num_outputs=*/</span><span class="number">0</span>);</span><br><span class="line">    jit::tracer::<span class="built_in">recordSourceLocation</span>(node);</span><br><span class="line">    jit::tracer::<span class="built_in">addInputs</span>(node, <span class="string">&quot;self&quot;</span>, self);</span><br><span class="line">    jit::tracer::<span class="built_in">addInputs</span>(node, <span class="string">&quot;other&quot;</span>, other);</span><br><span class="line">    jit::tracer::<span class="built_in">addInputs</span>(node, <span class="string">&quot;alpha&quot;</span>, alpha);</span><br><span class="line">    tracer_state-&gt;<span class="built_in">insertNode</span>(node);</span><br><span class="line"></span><br><span class="line">    jit::tracer::<span class="built_in">setTracingState</span>(<span class="literal">nullptr</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">auto</span> result =at::_ops::add_Tensor::<span class="built_in">redispatch</span>(ks &amp; c10::<span class="built_in">DispatchKeySet</span>(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, other, alpha);</span><br><span class="line">  <span class="keyword">if</span> (tracer_state) &#123;</span><br><span class="line">    jit::tracer::<span class="built_in">setTracingState</span>(std::<span class="built_in">move</span>(tracer_state));</span><br><span class="line">    jit::tracer::<span class="built_in">addOutput</span>(node, result);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在执行真实的 <code>add.Tensor</code> 操作之前会首先执行上述代码，在开启 trace 的条件下，会创建一个 op 名字为 <code>aten:add</code> 的节点，并设置其输入和输出，将其加入到静态图中，需要注意一点的是，上面捕获出来的算子是 <code>aten:add</code> 而不是 <code>aten:add.Tensor</code>，是因为其目的主要是为了方便后续 TorchScript IR 经由 ONNX Pass 转成 ONNX 模型。</p>
<p>优点：</p>
<ul>
<li>开箱即用</li>
<li>全图捕获</li>
<li>可以处理patch，动态修改等场景</li>
<li>不需要 fallback 机制</li>
</ul>
<p>缺点:</p>
<ul>
<li>不支持动态 shape（特化执行）</li>
<li>静态化数据控制流（特化执行）</li>
<li>不支持反向图捕获</li>
</ul>
<p>推荐使用场景：</p>
<ul>
<li>代码不涉及动态控制流</li>
<li>输入shape，类型固定，一次捕获，反复重放</li>
</ul>
<h4 id="torch-jit-script"><a href="#torch-jit-script" class="headerlink" title="torch.jit.script"></a>torch.jit.script</h4><p><code>torch.jit.script</code> 是在 Python 层面获取分析代码的抽象语法树（AST），进行解析分析，在运行之前捕获静态图，它的出现主要就是为了解决 <code>torch.jit.trace</code> 无法处理动态shape以及控制流的问题，但是鉴于 Python 语法极其灵活的特性，<code>torch.jit.script</code> 无法覆盖所有可能的场景，从而不可避免的出现各种问题。</p>
<p>优点：</p>
<ul>
<li>支持数据控制流</li>
<li>支持动态shape</li>
</ul>
<p>缺点:</p>
<ul>
<li>不支持反向图捕获</li>
<li>不支持patch，动态修改等场景</li>
<li>仅支持 Python 语法子集，易用性差</li>
<li>需要用户修改适配代码</li>
</ul>
<p>推荐使用场景：</p>
<ul>
<li>不推荐使用，最多作为 <code>torch.jit.trace</code> 补充使用</li>
</ul>
<h4 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h4><p>无论是通过 <code>torch.jit.trace</code> 还是 <code>torch.jit.script</code> 生成的 TorchScript IR，最终都可以通过 JIT 提供的解释器优化后解释执行，相关代码逻辑请参考<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/v2.7.1/torch/csrc/jit/runtime/interpreter.cpp">这里</a>。</p>
<h3 id="torch-fx"><a href="#torch-fx" class="headerlink" title="torch.fx"></a>torch.fx</h3><p><code>torch.fx</code> 是 PyTorch 于 v1.8.0 版本引入，严格意义上来说，<code>torch.fx</code> 的提出主要是便利模型量化，方便用户通过代码插入或者修改图节点，完成 Python 代码到 Python 代码的转换，它的功能入口点是在 C++ 算子的 Python 绑定入口，借助的是 PyTorch 拓展机制  <code>__torch_function__</code> 的能力以及符号跟踪机制来实现的。</p>
<blockquote>
<p>Note:<br>这里提到的 torch.fx 指代的是 symbolic_trace，而非 PyTorch 2.0 之后新出的 make_fx（AOT Autograd 的根基）。</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">static</span> PyObject * <span class="title">THPVariable_add</span><span class="params">(PyObject* self_, PyObject* args, PyObject* kwargs)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  HANDLE_TH_ERRORS</span><br><span class="line">  <span class="type">const</span> Tensor&amp; self = <span class="built_in">THPVariable_Unpack</span>(self_);</span><br><span class="line">  <span class="function"><span class="type">static</span> PythonArgParser <span class="title">parser</span><span class="params">(&#123;</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="string">&quot;add(Scalar alpha, Tensor other)|deprecated&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="string">&quot;add(Tensor other, *, Scalar alpha=1)&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">  &#125;, <span class="comment">/*traceable=*/</span><span class="literal">true</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">  ParsedArgs&lt;<span class="number">2</span>&gt; parsed_args;</span><br><span class="line">  <span class="keyword">auto</span> _r = parser.<span class="built_in">parse</span>(self_, args, kwargs, parsed_args);</span><br><span class="line">  <span class="keyword">if</span>(_r.<span class="built_in">has_torch_function</span>()) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">handle_torch_function</span>(_r, self_, args, kwargs, THPVariableClass, <span class="string">&quot;torch.Tensor&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">switch</span> (_r.idx) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span>: &#123;</span><br><span class="line">      <span class="comment">// [deprecated] aten::add(Tensor self, Scalar alpha, Tensor other) -&gt; Tensor</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">auto</span> dispatch_add = [](<span class="type">const</span> at::Tensor &amp; self, <span class="type">const</span> at::Scalar &amp; alpha, <span class="type">const</span> at::Tensor &amp; other) -&gt; at::Tensor &#123;</span><br><span class="line">        pybind11::gil_scoped_release no_gil;</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">add</span>(other, alpha);</span><br><span class="line">      &#125;;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">wrap</span>(<span class="built_in">dispatch_add</span>(self, _r.<span class="built_in">scalar</span>(<span class="number">0</span>), _r.<span class="built_in">tensor</span>(<span class="number">1</span>)));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>: &#123;</span><br><span class="line">      <span class="comment">// aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -&gt; Tensor</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">auto</span> dispatch_add = [](<span class="type">const</span> at::Tensor &amp; self, <span class="type">const</span> at::Tensor &amp; other, <span class="type">const</span> at::Scalar &amp; alpha) -&gt; at::Tensor &#123;</span><br><span class="line">        pybind11::gil_scoped_release no_gil;</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">add</span>(other, alpha);</span><br><span class="line">      &#125;;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">wrap</span>(<span class="built_in">dispatch_add</span>(self, _r.<span class="built_in">tensor</span>(<span class="number">0</span>), _r.<span class="built_in">scalar</span>(<span class="number">1</span>)));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  Py_RETURN_NONE;</span><br><span class="line">  END_HANDLE_TH_ERRORS</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>简单来说，PyTorch 分析 <code>function</code> 或者 <code>nn.Module</code> 获取输入信息，将输入信息包装成实现了 <code>__torch_function__</code> 的 <code>Proxy</code> 类型，将 <code>Proxy</code> 类型作为输入参数进行计算；PyTorch 层次的算子在遇到参数的类型带有 <code>__torch_function__</code> 属性的时候会触发上述代码中 <code>handle_torch_function</code> 的逻辑，最终将调用 <code>Proxy</code> 中定义的 <code>__torch_function__</code> 方法，该方法中会捕获 PyTorch 层面的算子操作，生成对应的 FX 图节点</p>
<p>优点：</p>
<ul>
<li>支持模型的动态修改、转换，并提供对应工具集</li>
<li>支持动态shape</li>
</ul>
<p>缺点:</p>
<ul>
<li>不支持动态数据流</li>
<li>不支持反向图捕获</li>
<li>需要用户修改适配代码</li>
</ul>
<p>推荐使用场景：</p>
<ul>
<li>量化场景</li>
</ul>
<h3 id="torch-lazy"><a href="#torch-lazy" class="headerlink" title="torch.lazy"></a>torch.lazy</h3><p><code>torch.lazy</code> 是 PyTorch 于 v1.10.0 版本引入的特性，该特性本身并不受广大用户了解，大多数用户了解更多的是基于它实现的 <code>torch_xla</code>, 其中心思想是运行过程中，利用 DispatchKey <code>Lazy</code> 动态捕获算子（需要实现加速器后端可以支持的所有算子的捕获逻辑），并将其记录到 IR 图中，而不是进行实际的计算；当遇到不支持的算子或者用户显示 <code>mark_step</code> 的时候，会触发捕获图的编译操作，生成捕获图对应的高效 kernel，从而加速整个运行过程。</p>
<p>PyTorch 仓库内部实现了一个简单的 <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/v2.7.1/torch/_lazy/ts_backend.py">ts_backend</a>，其原理与 <code>torch.xla</code> 基本完全一致，可以用作机制学习参考，可参见相关<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/v2.7.1/torch/csrc/lazy/tutorial.md">教程</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">at::Tensor <span class="title">LazyNativeFunctions::add</span><span class="params">(<span class="type">const</span> at::Tensor &amp;self, <span class="type">const</span> at::Tensor &amp;other, <span class="type">const</span> at::Scalar &amp;alpha)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (force_eager_fallback(at::aten::add))</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">return</span> at::native::call_fallback_fn_symint&lt;&amp;ltc_eager_fallback, <span class="built_in">ATEN_OP2</span>(add, Tensor)&gt;::<span class="built_in">call</span>(</span><br><span class="line">        self,</span><br><span class="line">        other,</span><br><span class="line">        alpha);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">TORCH_LAZY_FN_COUNTER</span>(<span class="string">&quot;lazy::&quot;</span>);</span><br><span class="line">  <span class="keyword">auto</span> common_device = torch::lazy::<span class="built_in">GetBackendDevice</span>(self, other);</span><br><span class="line">  <span class="built_in">TORCH_INTERNAL_ASSERT</span>(common_device);</span><br><span class="line"></span><br><span class="line">  LazyTensorPtr lazy_self = torch::lazy::<span class="built_in">GetLtcTensorOrCreateForWrappedNumber</span>(self, *common_device);</span><br><span class="line">  LazyTensorPtr lazy_other = torch::lazy::<span class="built_in">GetLtcTensorOrCreateForWrappedNumber</span>(other, *common_device);</span><br><span class="line">  <span class="keyword">auto</span> node_alpha = torch::lazy::LazyGraphExecutor::<span class="built_in">Get</span>()-&gt;<span class="built_in">GetIrValueForScalarFromCodegen</span>(alpha, *common_device);</span><br><span class="line">  torch::lazy::NodePtr node = torch::lazy::<span class="built_in">ReuseNode</span>&lt;AddTensor&gt;(lazy_self-&gt;<span class="built_in">GetIrValue</span>(), lazy_other-&gt;<span class="built_in">GetIrValue</span>(), node_alpha);</span><br><span class="line">  <span class="keyword">if</span> (!node)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">auto</span> self_meta = <span class="built_in">to_meta</span>(self);</span><br><span class="line">    <span class="keyword">auto</span> other_meta = <span class="built_in">to_meta</span>(other);</span><br><span class="line">    <span class="keyword">auto</span> out_meta = at::meta::<span class="built_in">add</span>(self_meta, other_meta, alpha);</span><br><span class="line"></span><br><span class="line">    std::vector&lt;torch::lazy::Shape&gt; shapes&#123;torch::lazy::<span class="built_in">Shape</span>(out_meta.<span class="built_in">scalar_type</span>(), out_meta.<span class="built_in">sizes</span>().<span class="built_in">vec</span>())&#125;;</span><br><span class="line">    <span class="built_in">TORCH_INTERNAL_ASSERT</span>(shapes.<span class="built_in">size</span>() == <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">if</span> (torch::lazy::<span class="built_in">symbolicShapeEnabled</span>())</span><br><span class="line">    &#123;</span><br><span class="line">      std::vector&lt;torch::jit::IValue&gt; inputs = &#123;self, other, alpha&#125;;</span><br><span class="line">      <span class="type">const</span> <span class="type">char</span> *schema_str = <span class="string">&quot;aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -&gt; Tensor&quot;</span>;</span><br><span class="line">      <span class="built_in">applySymbolicShapesOnLT</span>(schema_str, inputs, shapes);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    node = torch::lazy::<span class="built_in">MakeNode</span>&lt;AddTensor&gt;(lazy_self-&gt;<span class="built_in">GetIrValue</span>(), lazy_other-&gt;<span class="built_in">GetIrValue</span>(), node_alpha, std::<span class="built_in">move</span>(shapes));</span><br><span class="line">    <span class="built_in">CacheNode</span>(node);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> result = torch::lazy::<span class="built_in">CreateAtenFromLtcTensor</span>(</span><br><span class="line">      torch::lazy::LazyTensor::<span class="built_in">Create</span>(std::<span class="built_in">move</span>(node), *common_device));</span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>优点：</p>
<ul>
<li>开箱即用</li>
<li>自动完成子图拆分以及子图编译加速</li>
</ul>
<p>缺点:</p>
<ul>
<li>不支持反向图捕获</li>
<li>不支持动态shape（特化执行）</li>
<li>静态化数据控制流（特化执行）</li>
<li>不支持细粒度子图拆分，捕获子图重复度高，编译成本高</li>
</ul>
<p><code>torch.lazy</code> 与 <code>torch.jit.trace</code> 有着很高的相似程度，它们之间的差异性如下：</p>
<ul>
<li><code>torch.jit.trace</code> 每次都会生成特化后的整图（包括数据类型、Shape，以及控制流），由于控制流被特化，所以可能导致同一个导出模型在不同输入的情况下出现结果不对的情况，需要用户了解代码中是否有控制流等存在。</li>
<li><code>torch.lazy</code> 每次会缓存生成的编译图，当数据类型，Shape以及子图结构没有变化时复用编译图；如果有变化的时候就会重新构图，编译，执行以及缓存，实现了按需多次捕获，多次播放的功能。</li>
</ul>
<h3 id="torch-compiler-v2-0-0-引入"><a href="#torch-compiler-v2-0-0-引入" class="headerlink" title="torch.compiler(v2.0.0 引入)"></a>torch.compiler(v2.0.0 引入)</h3><p>从 PyTorch 1.0.0 到 1.10.0，PyTorch 编译器团队先后尝试了 <code>torch.jit</code>、<code>torch.fx</code> 以及 <code>torch.lazy</code>，但是他们都有各自的优势，以及各自的缺陷，事实证明，需要一种更加高效，易用性更好的方式来解决上面所有的这些问题；</p>
<p>在这里简单罗列下，新的机制需要解决的问题：</p>
<ul>
<li>开箱即用，用户无感(<code>torch.lazy</code> 完全支持, <code>torch.jit.trace</code> 需要用户了解生成模型的局限性)</li>
<li>支持代码 patch，动态注入（源码层面语法解析走不通）（<code>torch.lazy</code>、<code>torch.jit.trace</code> 以及 <code>torch.fx</code> 均支持）</li>
<li>动态 shape 支持，避免反复图捕获、图编译代价(<code>torch.jit.script</code>， <code>torch.fx</code> 支持)</li>
<li>数据依赖控制流支持（<code>torch.jit.script</code> 支持）</li>
<li>支持捕获反向图</li>
<li>支持细粒度子图拆分</li>
<li>支持更深层次的源码级算子融合，而不是基于规则的融合</li>
<li>…</li>
</ul>
<p>鉴于上述问题， PyTorch compile 图模式横空出世，将 PyTorch 从 1.0 直接跃升成 2.0，下一篇文章将聚焦介绍基于收敛后字节码分析的 <code>torch dynamo</code> 模块的原理。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://fffrog.github.io/mrl">Jiawei Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://fffrog.github.io/mrl/2025/06/16/Advanced-PyTorch-Features-Graph-Mode/">https://fffrog.github.io/mrl/2025/06/16/Advanced-PyTorch-Features-Graph-Mode/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/mrl/tags/Compiler/">Compiler</a></div><div class="post_share"><div class="social-share" data-image="https://i.mji.rip/2023/11/08/d1245194556cd938b7176c269d1adacf.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/mrl/2025/01/20/The-Introduction-of-PyTorch-Compiler/" title="The Introduction of PyTorch Compiler"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">The Introduction of PyTorch Compiler</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/mrl/2025/01/20/The-Introduction-of-PyTorch-Compiler/" title="The Introduction of PyTorch Compiler"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-20</div><div class="title">The Introduction of PyTorch Compiler</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.mji.rip/2023/11/08/d1245194556cd938b7176c269d1adacf.jpeg" onerror="this.onerror=null;this.src='/mrl/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Jiawei Li</div><div class="author-info__description">Stay Hungry, Stay Foolish</div></div><div class="card-info-data site-data is-center"><a href="/mrl/archives/"><div class="headline">Articles</div><div class="length-num">7</div></a><a href="/mrl/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/mrl/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/FFFrog" target="_blank" title="Github"><i class="fab fa-github" style="color: #hdhfbb;"></i></a><a class="social-icon" href="mailto:ljw1101.vip@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #000000;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Happy Everyday</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F"><span class="toc-number">2.</span> <span class="toc-text">图模式的前世今生</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-jit"><span class="toc-number">2.1.</span> <span class="toc-text">torch.jit</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-jit-trace"><span class="toc-number">2.1.1.</span> <span class="toc-text">torch.jit.trace</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-jit-script"><span class="toc-number">2.1.2.</span> <span class="toc-text">torch.jit.script</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C"><span class="toc-number">2.1.3.</span> <span class="toc-text">运行</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-fx"><span class="toc-number">2.2.</span> <span class="toc-text">torch.fx</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-lazy"><span class="toc-number">2.3.</span> <span class="toc-text">torch.lazy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-compiler-v2-0-0-%E5%BC%95%E5%85%A5"><span class="toc-number">2.4.</span> <span class="toc-text">torch.compiler(v2.0.0 引入)</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/mrl/2025/06/16/Advanced-PyTorch-Features-Graph-Mode/" title="Understanding PyTorch Graph Mode">Understanding PyTorch Graph Mode</a><time datetime="2025-06-16T11:12:42.000Z" title="Created 2025-06-16 19:12:42">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/mrl/2025/01/20/The-Introduction-of-PyTorch-Compiler/" title="The Introduction of PyTorch Compiler">The Introduction of PyTorch Compiler</a><time datetime="2025-01-20T02:20:00.000Z" title="Created 2025-01-20 10:20:00">2025-01-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/mrl/2023/11/13/PyTorch-AMP-Mechanism/" title="PyTorch AMP Mechanism">PyTorch AMP Mechanism</a><time datetime="2023-11-13T02:39:33.000Z" title="Created 2023-11-13 10:39:33">2023-11-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/mrl/2023/11/09/PyTorch-Tensor-Mechanism/" title="PyTorch Tensor Mechanism">PyTorch Tensor Mechanism</a><time datetime="2023-11-09T02:30:48.000Z" title="Created 2023-11-09 10:30:48">2023-11-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/mrl/2023/11/09/PyTorch-Operator-Register/" title="PyTorch Operator Register">PyTorch Operator Register</a><time datetime="2023-11-09T02:27:53.000Z" title="Created 2023-11-09 10:27:53">2023-11-09</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Jiawei Li</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/mrl/js/utils.js"></script><script src="/mrl/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>